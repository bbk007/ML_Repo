{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "# https://medium.com/datadriveninvestor/choosing-the-best-algorithm-for-your-classification-model-7c632c78f38f\n",
    "# https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "# https://www.dataquest.io/blog/sci-kit-learn-tutorial/\n",
    "# https://datascience.stackexchange.com/questions/33256/how-to-apply-machine-learning-model-to-new-dataset\n",
    "# https://speakerdeck.com/datasciencela/tianqi-chen-xgboost-overview-and-latest-news-la-meetup-talk?slide=33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular import *\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn import svm, tree\n",
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "\n",
    "from sklearn import model_selection\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "import xgboost as xgb\n",
    "\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/Users/bbabu/fastai/DataSamples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = pd.read_csv(path/'Output-5v5.csv', sep=',', header=0)\n",
    "df_test = pd.read_csv(path/'Output-5v5-Test.csv', sep=',', header=0)\n",
    "df_test_2 = pd.read_csv(path/'Output-5v5-Test.csv', sep=',', header=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.drop(['Description', 'Platform', 'BusinessGroup', 'Country'], axis=1, inplace=True)\n",
    "df_test.drop(['Description', 'Platform', 'BusinessGroup', 'Country'], axis=1, inplace=True)\n",
    "df_test_2.drop(['Description', 'Platform', 'BusinessGroup', 'Country'], axis=1, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path/'Groups_with_Owners-V2.csv', sep=',', header=0)\n",
    "df_test = pd.read_csv(path/'Groups_without_Owners-V2.csv', sep=',', header=0)\n",
    "df_test_2 = pd.read_csv(path/'Groups_without_Owners-V2.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(path/'Groups_with_Owners-V1.csv', sep=';', header=0)\n",
    "# df_test = pd.read_csv(path/'Groups_without_Owners-V1.csv', sep=';', header=0)\n",
    "# df_test_2 = pd.read_csv(path/'Groups_without_Owners-V1.csv', sep=';', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Description', 'MemberDN', 'State'], axis=1, inplace=True)\n",
    "df_test.drop(['Description', 'MemberDN', 'State', 'Owner'], axis=1, inplace=True)\n",
    "df_test_2.drop(['Description', 'MemberDN', 'State', 'Owner'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.drop(['GroupCountry', 'GroupDomain', 'Domain'], axis=1, inplace=True)\n",
    "# df_test.drop(['GroupCountry', 'GroupDomain', 'Domain'], axis=1, inplace=True)\n",
    "# df_test_2.drop(['GroupCountry', 'GroupDomain', 'Domain'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['GroupCountry'], axis=1, inplace=True)\n",
    "df_test.drop(['GroupCountry'], axis=1, inplace=True)\n",
    "df_test_2.drop(['GroupCountry'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(keep='first',inplace=True) \n",
    "df_test.drop_duplicates(keep='first',inplace=True)\n",
    "df_test_2.drop_duplicates(keep='first',inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.dtypes)\n",
    "# print(df_test.dtypes)\n",
    "# print(df_test_2.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "categorical = list(data.select_dtypes(include=['object']).columns.values)\n",
    "for cat in categorical:\n",
    "    #print(cat)\n",
    "    data[cat].fillna('NaN', inplace=True)\n",
    "    data[cat] = le.fit_transform(data[cat].astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "categorical = list(df_test_2.select_dtypes(include=['object']).columns.values)\n",
    "for cat in categorical:\n",
    "    #print(cat)\n",
    "    df_test_2[cat].fillna('NaN', inplace=True)\n",
    "    df_test_2[cat] = le.fit_transform(df_test_2[cat].astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.set_index('GroupDN', drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dependent and Independent Datasets based on our Dependent #and Independent features\n",
    "# X = data[['GroupDN', 'Member', 'Manager', 'BusinessGroup', 'Platform', 'Title', 'Department', 'Country', 'City', 'GroupCountry', 'GroupDomain', 'Domain']]\n",
    "X = data[['GroupDN', 'Member', 'Manager', 'BusinessGroup', 'Platform', 'Title', 'Department', 'Country', 'City', 'GroupDomain', 'Domain']]\n",
    "# X = data[['GroupDN', 'Member', 'Manager', 'BusinessGroup', 'Platform', 'Title', 'Department', 'Country', 'City']]\n",
    "y = data['Owner']\n",
    "#Split the Data into Training and Testing sets with test size as #33%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33, shuffle=False)\n",
    "eval_set = [(X_train, y_train), (X_test, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.reindex(columns=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:47:39] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4384 extra nodes, 182 pruned nodes, max_depth=26\n",
      "[15:47:40] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4364 extra nodes, 200 pruned nodes, max_depth=26\n",
      "[15:47:41] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4378 extra nodes, 184 pruned nodes, max_depth=26\n",
      "[15:47:42] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4374 extra nodes, 186 pruned nodes, max_depth=26\n",
      "[15:47:43] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4328 extra nodes, 158 pruned nodes, max_depth=26\n",
      "[15:47:44] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4318 extra nodes, 146 pruned nodes, max_depth=26\n",
      "[15:47:45] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4226 extra nodes, 148 pruned nodes, max_depth=26\n",
      "[15:47:46] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4166 extra nodes, 166 pruned nodes, max_depth=26\n",
      "[15:47:48] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4174 extra nodes, 150 pruned nodes, max_depth=26\n",
      "[15:47:49] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4156 extra nodes, 160 pruned nodes, max_depth=26\n",
      "[15:47:50] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4090 extra nodes, 166 pruned nodes, max_depth=26\n",
      "[15:47:51] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4098 extra nodes, 140 pruned nodes, max_depth=26\n",
      "[15:47:52] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4114 extra nodes, 136 pruned nodes, max_depth=26\n",
      "[15:47:53] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4084 extra nodes, 116 pruned nodes, max_depth=26\n",
      "[15:47:54] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4084 extra nodes, 112 pruned nodes, max_depth=26\n",
      "[15:47:55] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4094 extra nodes, 132 pruned nodes, max_depth=26\n",
      "[15:47:57] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4090 extra nodes, 134 pruned nodes, max_depth=26\n",
      "[15:47:58] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4080 extra nodes, 138 pruned nodes, max_depth=26\n",
      "[15:47:59] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4090 extra nodes, 116 pruned nodes, max_depth=26\n",
      "[15:48:00] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4070 extra nodes, 118 pruned nodes, max_depth=26\n",
      "[15:48:01] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4094 extra nodes, 102 pruned nodes, max_depth=26\n",
      "[15:48:02] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4092 extra nodes, 110 pruned nodes, max_depth=26\n",
      "[15:48:03] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4078 extra nodes, 108 pruned nodes, max_depth=26\n",
      "[15:48:04] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4094 extra nodes, 106 pruned nodes, max_depth=26\n",
      "[15:48:06] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4090 extra nodes, 102 pruned nodes, max_depth=26\n",
      "[15:48:07] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4082 extra nodes, 108 pruned nodes, max_depth=26\n",
      "[15:48:08] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4064 extra nodes, 108 pruned nodes, max_depth=26\n",
      "[15:48:09] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4084 extra nodes, 94 pruned nodes, max_depth=26\n",
      "[15:48:10] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4058 extra nodes, 110 pruned nodes, max_depth=26\n",
      "[15:48:11] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4074 extra nodes, 100 pruned nodes, max_depth=26\n",
      "[15:48:12] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4040 extra nodes, 108 pruned nodes, max_depth=26\n",
      "[15:48:13] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4078 extra nodes, 94 pruned nodes, max_depth=26\n",
      "[15:48:14] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4058 extra nodes, 96 pruned nodes, max_depth=26\n",
      "[15:48:15] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4066 extra nodes, 112 pruned nodes, max_depth=26\n",
      "[15:48:17] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4058 extra nodes, 106 pruned nodes, max_depth=26\n",
      "[15:48:18] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4100 extra nodes, 120 pruned nodes, max_depth=26\n",
      "[15:48:19] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4060 extra nodes, 94 pruned nodes, max_depth=26\n",
      "[15:48:20] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4108 extra nodes, 108 pruned nodes, max_depth=26\n",
      "[15:48:21] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4070 extra nodes, 104 pruned nodes, max_depth=26\n",
      "[15:48:22] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4090 extra nodes, 106 pruned nodes, max_depth=26\n",
      "[15:48:23] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4096 extra nodes, 102 pruned nodes, max_depth=26\n",
      "[15:48:25] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4066 extra nodes, 90 pruned nodes, max_depth=26\n",
      "[15:48:26] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4102 extra nodes, 104 pruned nodes, max_depth=26\n",
      "[15:48:27] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4102 extra nodes, 100 pruned nodes, max_depth=26\n",
      "[15:48:28] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4058 extra nodes, 90 pruned nodes, max_depth=26\n",
      "[15:48:29] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4104 extra nodes, 100 pruned nodes, max_depth=26\n",
      "[15:48:30] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4106 extra nodes, 96 pruned nodes, max_depth=26\n",
      "[15:48:31] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4058 extra nodes, 90 pruned nodes, max_depth=26\n",
      "[15:48:33] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4100 extra nodes, 96 pruned nodes, max_depth=26\n",
      "[15:48:34] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4098 extra nodes, 96 pruned nodes, max_depth=26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0.2,\n",
       "              learning_rate=0.0001, max_delta_step=0, max_depth=50,\n",
       "              min_child_weight=1, missing=None, n_estimators=50, n_jobs=500,\n",
       "              nthread=1000, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgboost.XGBClassifier(learning_rate=0.0001, \n",
    "                              max_depth=50, \n",
    "                              n_estimators=50, \n",
    "#                               seed=14,\n",
    "#                               eval_metric='error',\n",
    "                              booster='gbtree', \n",
    "                              objective='binary:logistic',\n",
    "                              base_score=0.5,\n",
    "                              colsample_bylevel=1,\n",
    "                              colsample_bynode=1, \n",
    "                              colsample_bytree=1, \n",
    "                              gamma=0.2,\n",
    "                              max_delta_step=0, \n",
    "                              min_child_weight=1, \n",
    "                              missing=None, \n",
    "                              n_jobs=500,\n",
    "                              nthread=1000, \n",
    "                              random_state=0,\n",
    "                              reg_alpha=0, \n",
    "                              reg_lambda=1, \n",
    "                              scale_pos_weight=1, \n",
    "                              subsample=1, \n",
    "                              verbosity=3)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "logloss = log_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to disk\n",
    "filename = path/'Final_Model-XGBoosterV6.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = path/'Final_Model-XGBooster-Saved.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from disk and use it to make new predictions\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "# result = loaded_model.score(X_test, y_test)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:48:35] ======== Monitor:  ========\n",
      "[15:48:35] ======== Monitor:  ========\n"
     ]
    }
   ],
   "source": [
    "X_test = df_test_2\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "# X_test = sc.transform(X_test)\n",
    "pred = loaded_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_score = pd.Series(loaded_model.get_booster().get_fscore(),index=df_test_2.columns.values).sort_values(ascending=False)\n",
    "# print(f_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cover = pd.Series(loaded_model.get_booster().get_score(importance_type=\"cover\"),index=df_test_2.columns.values).sort_values(ascending=False)\n",
    "# print(cover)\n",
    "# gain = pd.Series(loaded_model.get_booster().get_score(importance_type=\"gain\"),index=df_test_2.columns.values).sort_values(ascending=False)\n",
    "# print(gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost.plot_tree(loaded_model, rankdir='LR')\n",
    "# # plt.show()\n",
    "# fig = plt.gcf()\n",
    "# fig.set_size_inches(400, 300)\n",
    "# fig.savefig(path/'Tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_importance(model)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.Series(loaded_model.feature_importances_,index=df_test_2.columns.values).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test['Member'] = le.inverse_transform(df_test['Member'])\n",
    "final_df = pd.DataFrame({'GroupDN': df_test['GroupDN'],'Member': df_test['Member'], 'Manager': df_test['Manager'], 'Owner': pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(path/'MLProcessed1-XGBoosterV6.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = final_df.groupby(['GroupDN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for name, group in grouped:\n",
    "    if group['Owner'].sum() == 0:\n",
    "        i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance table is as follows:\n",
      "Manager          0.882230\n",
      "BusinessGroup    0.028993\n",
      "Domain           0.014244\n",
      "Department       0.012571\n",
      "Platform         0.010790\n",
      "Member           0.009822\n",
      "Country          0.009321\n",
      "GroupDomain      0.009144\n",
      "Title            0.008653\n",
      "City             0.008462\n",
      "GroupDN          0.005768\n",
      "Model accuracy is: 0.9398477143346199\n",
      "Model logloss is: 2.0775917968327193\n",
      "Total number of predicted owners are: 88426\n",
      "Mean of owners are: 0.09724847928963498\n",
      "Total number of groups are: 48963\n",
      "Number of predicted groups with owners are: 14315\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature importance table is as follows:\")\n",
    "print(feature_imp.to_string())\n",
    "# print(pred)\n",
    "print(\"Model accuracy is: %s\"%(acc))\n",
    "print(\"Model logloss is: %s\"%(logloss))\n",
    "print(\"Total number of predicted owners are: %s\"%(pred.sum()))\n",
    "print(\"Mean of owners are: %s\"%(pred.mean()))\n",
    "print(\"Total number of groups are: %s\"%(len(grouped)))\n",
    "print(\"Number of predicted groups with owners are: %s\"%(len(grouped) - i))\n",
    "# print(\"Accuracy score of the model is: %s\"%(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test_2.to_csv(path/'Encoded_without_Owners.csv', header=True, index=False)\n",
    "# data.to_csv(path/'Encoded_with_Owners.csv', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
